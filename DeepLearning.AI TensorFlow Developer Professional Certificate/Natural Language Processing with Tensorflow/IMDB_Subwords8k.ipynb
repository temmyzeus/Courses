{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB_Subwords8k.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN1StNKCnk1VHt8/CYTXb/V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/temmyzeus/Tensorflow-Courses/blob/master/DeepLearning.AI%20TensorFlow%20Developer%20Professional%20Certificate/Natural%20Language%20Processing%20with%20Tensorflow/IMDB_Subwords8k.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m65aO4yhBIT1"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Zr1lnRZCiHx",
        "outputId": "0a21f625-e940-4bcc-e755-e62f7b8c381b"
      },
      "source": [
        "imdb, info = tfds.load(\n",
        "    'imdb_reviews/subwords8k',\n",
        "    with_info=True,\n",
        "    as_supervised=True\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvmnonsJCoqE"
      },
      "source": [
        "train = imdb['train']\n",
        "test = imdb['test']\n",
        "# Get Subword Text Encoder\n",
        "tokenizer = info.features['text'].encoder"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afi-8kRkHR5W",
        "outputId": "d594adfc-dd5b-48f4-9d80-5e9c7b10b3d4"
      },
      "source": [
        "# convert sample_tokens to text\n",
        "sample_text = 'I\\'m going to be a very good ML Engineer'\n",
        "sample_tokens = tokenizer.encode(sample_text)\n",
        "\n",
        "print(tokenizer.encode(sample_text), end='\\n\\n')\n",
        "print(tokenizer.decode(sample_tokens))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8002, 7968, 111, 215, 7, 35, 4, 67, 74, 8006, 3780, 1844, 1004, 1225, 8043]\n",
            "\n",
            "I'm going to be a very good ML Engineer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQC7vbrTcGiB",
        "outputId": "153988d8-782c-4937-e75f-ec12a70fc1a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "# Getting an Idea of teh distribution of token lengths\n",
        "train_lens = np.array([len(_[0]) for _ in train])\n",
        "print('Min Len :', np.min(train_lens))\n",
        "print('Max Len: ', np.max(train_lens))\n",
        "# Getting a good sense of the middle point, median seems better here\n",
        "print('Median Len', np.median(train_lens))\n",
        "plt.hist(train_lens, bins=30)\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min Len : 11\n",
            "Max Len:  3944\n",
            "Median Len 253.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AWMNfaIfIok"
      },
      "source": [
        "vocab_size = tokenizer.vocab_size\n",
        "output_dim = 20\n",
        "input_length = 500\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hphWOBNRnamK",
        "outputId": "3603d1e9-c73e-4d53-9966-6461e99c966a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tf.compat.v1.data.get_output_shapes(train)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([None]), TensorShape([]))"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwUkTQvBpEIr",
        "outputId": "487ad092-ccae-4e43-a417-2708997c0dd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((None,), ()), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuSo3lDIjMyw",
        "outputId": "48b08086-452a-4511-b36e-11dc4934c769",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "train.padded_batch(\n",
        "    batch_size=BATCH_SIZE,\n",
        "    padded_shapes=\n",
        ")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_padded_shape_to_tensor\u001b[0;34m(padded_shape, input_component_shape)\u001b[0m\n\u001b[1;32m   4770\u001b[0m         [dim if dim is not None else -1\n\u001b[0;32m-> 4771\u001b[0;31m          for dim in padded_shape_as_shape.as_list()], dtype=dtypes.int64)\n\u001b[0m\u001b[1;32m   4772\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_list\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1216\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"as_list() is not defined on an unknown TensorShape.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: as_list() is not defined on an unknown TensorShape.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-fc6d8887d58b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     padded_shapes=(\n\u001b[1;32m      4\u001b[0m         \u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         None)\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mpadded_batch\u001b[0;34m(self, batch_size, padded_shapes, padding_values, drop_remainder)\u001b[0m\n\u001b[1;32m   1705\u001b[0m                          \"input has an unknown rank\")\n\u001b[1;32m   1706\u001b[0m     return PaddedBatchDataset(self, batch_size, padded_shapes, padding_values,\n\u001b[0;32m-> 1707\u001b[0;31m                               drop_remainder)\n\u001b[0m\u001b[1;32m   1708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, batch_size, padded_shapes, padding_values, drop_remainder)\u001b[0m\n\u001b[1;32m   4874\u001b[0m         nest.flatten(input_shapes), flat_padded_shapes):\n\u001b[1;32m   4875\u001b[0m       flat_padded_shapes_as_tensors.append(\n\u001b[0;32m-> 4876\u001b[0;31m           _padded_shape_to_tensor(padded_shape, input_component_shape))\n\u001b[0m\u001b[1;32m   4877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4878\u001b[0m     self._padded_shapes = nest.pack_sequence_as(input_shapes,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_padded_shape_to_tensor\u001b[0;34m(padded_shape, input_component_shape)\u001b[0m\n\u001b[1;32m   4774\u001b[0m     \u001b[0;31m# `tf.TensorShape`, so fall back on the conversion to tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4775\u001b[0m     \u001b[0;31m# machinery.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4776\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4777\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4778\u001b[0m       six.reraise(ValueError, ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    344\u001b[0m                                          as_ref=False):\n\u001b[1;32m    345\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    270\u001b[0m   \"\"\"\n\u001b[1;32m    271\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 272\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m   \u001b[0;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1snCHaTDe7r7",
        "outputId": "511c9ce2-6c4e-42d9-98a2-596e44e1c3ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_ds = train.shuffle(buffer_size=BUFFER_SIZE)\n",
        "train_ds.padded_batch"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DatasetV2.padded_batch of <ShuffleDataset shapes: ((None,), ()), types: (tf.int64, tf.int64)>>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Zd3ikPoe7md",
        "outputId": "65d18168-4568-4463-e06b-a9c545c7aa27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenizer.subwords"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the_',\n",
              " ', ',\n",
              " '. ',\n",
              " 'a_',\n",
              " 'and_',\n",
              " 'of_',\n",
              " 'to_',\n",
              " 's_',\n",
              " 'is_',\n",
              " 'br',\n",
              " 'in_',\n",
              " 'I_',\n",
              " 'that_',\n",
              " 'this_',\n",
              " 'it_',\n",
              " ' /><',\n",
              " ' />',\n",
              " 'was_',\n",
              " 'The_',\n",
              " 'as_',\n",
              " 't_',\n",
              " 'with_',\n",
              " 'for_',\n",
              " '.<',\n",
              " 'on_',\n",
              " 'but_',\n",
              " 'movie_',\n",
              " ' (',\n",
              " 'are_',\n",
              " 'his_',\n",
              " 'have_',\n",
              " 'film_',\n",
              " 'not_',\n",
              " 'ing_',\n",
              " 'be_',\n",
              " 'ed_',\n",
              " 'you_',\n",
              " ' \"',\n",
              " 'it',\n",
              " 'd_',\n",
              " 'an_',\n",
              " 'he_',\n",
              " 'by_',\n",
              " 'at_',\n",
              " 'one_',\n",
              " 'who_',\n",
              " 'y_',\n",
              " 'from_',\n",
              " 'e_',\n",
              " 'or_',\n",
              " 'all_',\n",
              " 'like_',\n",
              " 'they_',\n",
              " '\" ',\n",
              " 'so_',\n",
              " 'just_',\n",
              " 'has_',\n",
              " ') ',\n",
              " 'her_',\n",
              " 'about_',\n",
              " 'out_',\n",
              " 'This_',\n",
              " 'some_',\n",
              " 'ly_',\n",
              " 'movie',\n",
              " 'film',\n",
              " 'very_',\n",
              " 'more_',\n",
              " 'It_',\n",
              " 'would_',\n",
              " 'what_',\n",
              " 'when_',\n",
              " 'which_',\n",
              " 'good_',\n",
              " 'if_',\n",
              " 'up_',\n",
              " 'only_',\n",
              " 'even_',\n",
              " 'their_',\n",
              " 'had_',\n",
              " 'really_',\n",
              " 'my_',\n",
              " 'can_',\n",
              " 'no_',\n",
              " 'were_',\n",
              " 'see_',\n",
              " 'she_',\n",
              " '? ',\n",
              " 'than_',\n",
              " '! ',\n",
              " 'there_',\n",
              " 'get_',\n",
              " 'been_',\n",
              " 'into_',\n",
              " ' - ',\n",
              " 'will_',\n",
              " 'much_',\n",
              " 'story_',\n",
              " 'because_',\n",
              " 'ing',\n",
              " 'time_',\n",
              " 'n_',\n",
              " 'we_',\n",
              " 'ed',\n",
              " 'me_',\n",
              " ': ',\n",
              " 'most_',\n",
              " 'other_',\n",
              " 'don',\n",
              " 'do_',\n",
              " 'm_',\n",
              " 'es_',\n",
              " 'how_',\n",
              " 'also_',\n",
              " 'make_',\n",
              " 'its_',\n",
              " 'could_',\n",
              " 'first_',\n",
              " 'any_',\n",
              " \"' \",\n",
              " 'people_',\n",
              " 'great_',\n",
              " 've_',\n",
              " 'ly',\n",
              " 'er_',\n",
              " 'made_',\n",
              " 'r_',\n",
              " 'But_',\n",
              " 'think_',\n",
              " \" '\",\n",
              " 'i_',\n",
              " 'bad_',\n",
              " 'A_',\n",
              " 'And_',\n",
              " 'It',\n",
              " 'on',\n",
              " '; ',\n",
              " 'him_',\n",
              " 'being_',\n",
              " 'never_',\n",
              " 'way_',\n",
              " 'that',\n",
              " 'many_',\n",
              " 'then_',\n",
              " 'where_',\n",
              " 'two_',\n",
              " 'In_',\n",
              " 'after_',\n",
              " 'too_',\n",
              " 'little_',\n",
              " 'you',\n",
              " '), ',\n",
              " 'well_',\n",
              " 'ng_',\n",
              " 'your_',\n",
              " 'If_',\n",
              " 'l_',\n",
              " '). ',\n",
              " 'does_',\n",
              " 'ever_',\n",
              " 'them_',\n",
              " 'did_',\n",
              " 'watch_',\n",
              " 'know_',\n",
              " 'seen_',\n",
              " 'time',\n",
              " 'er',\n",
              " 'character_',\n",
              " 'over_',\n",
              " 'characters_',\n",
              " 'movies_',\n",
              " 'man_',\n",
              " 'There_',\n",
              " 'love_',\n",
              " 'best_',\n",
              " 'still_',\n",
              " 'off_',\n",
              " 'such_',\n",
              " 'in',\n",
              " 'should_',\n",
              " 'the',\n",
              " 're_',\n",
              " 'He_',\n",
              " 'plot_',\n",
              " 'films_',\n",
              " 'go_',\n",
              " 'these_',\n",
              " 'acting_',\n",
              " 'doesn',\n",
              " 'es',\n",
              " 'show_',\n",
              " 'through_',\n",
              " 'better_',\n",
              " 'al_',\n",
              " 'something_',\n",
              " 'didn',\n",
              " 'back_',\n",
              " 'those_',\n",
              " 'us_',\n",
              " 'less_',\n",
              " '...',\n",
              " 'say_',\n",
              " 'is',\n",
              " 'one',\n",
              " 'makes_',\n",
              " 'and',\n",
              " 'can',\n",
              " 'all',\n",
              " 'ion_',\n",
              " 'find_',\n",
              " 'scene_',\n",
              " 'old_',\n",
              " 'real_',\n",
              " 'few_',\n",
              " 'going_',\n",
              " 'well',\n",
              " 'actually_',\n",
              " 'watching_',\n",
              " 'life_',\n",
              " 'me',\n",
              " '. <',\n",
              " 'o_',\n",
              " 'man',\n",
              " 'there',\n",
              " 'scenes_',\n",
              " 'same_',\n",
              " 'he',\n",
              " 'end_',\n",
              " 'this',\n",
              " '... ',\n",
              " 'k_',\n",
              " 'while_',\n",
              " 'thing_',\n",
              " 'of',\n",
              " 'look_',\n",
              " 'quite_',\n",
              " 'out',\n",
              " 'lot_',\n",
              " 'want_',\n",
              " 'why_',\n",
              " 'seems_',\n",
              " 'every_',\n",
              " 'll_',\n",
              " 'pretty_',\n",
              " 'got_',\n",
              " 'able_',\n",
              " 'nothing_',\n",
              " 'good',\n",
              " 'As_',\n",
              " 'story',\n",
              " ' & ',\n",
              " 'another_',\n",
              " 'take_',\n",
              " 'to',\n",
              " 'years_',\n",
              " 'between_',\n",
              " 'give_',\n",
              " 'am_',\n",
              " 'work_',\n",
              " 'isn',\n",
              " 'part_',\n",
              " 'before_',\n",
              " 'actors_',\n",
              " 'may_',\n",
              " 'gets_',\n",
              " 'young_',\n",
              " 'down_',\n",
              " 'around_',\n",
              " 'ng',\n",
              " 'thought_',\n",
              " 'though_',\n",
              " 'end',\n",
              " 'without_',\n",
              " 'What_',\n",
              " 'They_',\n",
              " 'things_',\n",
              " 'life',\n",
              " 'always_',\n",
              " 'must_',\n",
              " 'cast_',\n",
              " 'almost_',\n",
              " 'h_',\n",
              " '10',\n",
              " 'saw_',\n",
              " 'own_',\n",
              " 'here',\n",
              " 'bit_',\n",
              " 'come_',\n",
              " 'both_',\n",
              " 'might_',\n",
              " 'g_',\n",
              " 'whole_',\n",
              " 'new_',\n",
              " 'director_',\n",
              " 'them',\n",
              " 'horror_',\n",
              " 'ce',\n",
              " 'You_',\n",
              " 'least_',\n",
              " 'bad',\n",
              " 'big_',\n",
              " 'enough_',\n",
              " 'him',\n",
              " 'feel_',\n",
              " 'probably_',\n",
              " 'up',\n",
              " 'here_',\n",
              " 'making_',\n",
              " 'long_',\n",
              " 'her',\n",
              " 'st_',\n",
              " 'kind_',\n",
              " '--',\n",
              " 'original_',\n",
              " 'fact_',\n",
              " 'rather_',\n",
              " 'or',\n",
              " 'far_',\n",
              " 'nt_',\n",
              " 'played_',\n",
              " 'found_',\n",
              " 'last_',\n",
              " 'movies',\n",
              " 'When_',\n",
              " 'so',\n",
              " '\", ',\n",
              " 'comes_',\n",
              " 'action_',\n",
              " 'She_',\n",
              " 've',\n",
              " 'our_',\n",
              " 'anything_',\n",
              " 'funny_',\n",
              " 'ion',\n",
              " 'right_',\n",
              " 'way',\n",
              " 'trying_',\n",
              " 'now_',\n",
              " 'ous_',\n",
              " 'each_',\n",
              " 'done_',\n",
              " 'since_',\n",
              " 'ic_',\n",
              " 'point_',\n",
              " '\". ',\n",
              " 'wasn',\n",
              " 'interesting_',\n",
              " 'c_',\n",
              " 'worst_',\n",
              " 'te_',\n",
              " 'le_',\n",
              " 'ble_',\n",
              " 'ty_',\n",
              " 'looks_',\n",
              " 'show',\n",
              " 'put_',\n",
              " 'looking_',\n",
              " 'especially_',\n",
              " 'believe_',\n",
              " 'en_',\n",
              " 'goes_',\n",
              " 'over',\n",
              " 'ce_',\n",
              " 'p_',\n",
              " 'films',\n",
              " 'hard_',\n",
              " 'main_',\n",
              " 'be',\n",
              " 'having_',\n",
              " 'ry',\n",
              " 'TV_',\n",
              " 'worth_',\n",
              " 'One_',\n",
              " 'do',\n",
              " 'al',\n",
              " 're',\n",
              " 'again',\n",
              " 'series_',\n",
              " 'takes_',\n",
              " 'guy_',\n",
              " 'family_',\n",
              " 'seem_',\n",
              " 'plays_',\n",
              " 'role_',\n",
              " 'away_',\n",
              " 'world_',\n",
              " 'My_',\n",
              " 'character',\n",
              " ', \"',\n",
              " 'performance_',\n",
              " '2_',\n",
              " 'So_',\n",
              " 'watched_',\n",
              " 'John_',\n",
              " 'th_',\n",
              " 'plot',\n",
              " 'script_',\n",
              " 'For_',\n",
              " 'sure_',\n",
              " 'characters',\n",
              " 'set_',\n",
              " 'different_',\n",
              " 'minutes_',\n",
              " 'All_',\n",
              " 'American_',\n",
              " 'anyone_',\n",
              " 'Not_',\n",
              " 'music_',\n",
              " 'ry_',\n",
              " 'shows_',\n",
              " 'too',\n",
              " 'son_',\n",
              " 'en',\n",
              " 'day_',\n",
              " 'use_',\n",
              " 'someone_',\n",
              " 'for',\n",
              " 'woman_',\n",
              " 'yet_',\n",
              " '.\" ',\n",
              " 'during_',\n",
              " 'she',\n",
              " 'ro',\n",
              " '- ',\n",
              " 'times_',\n",
              " 'left_',\n",
              " 'used_',\n",
              " 'le',\n",
              " 'three_',\n",
              " 'play_',\n",
              " 'work',\n",
              " 'ness_',\n",
              " 'We_',\n",
              " 'girl_',\n",
              " 'comedy_',\n",
              " 'ment_',\n",
              " 'an',\n",
              " 'simply_',\n",
              " 'off',\n",
              " 'ies_',\n",
              " 'funny',\n",
              " 'ne',\n",
              " 'acting',\n",
              " 'That_',\n",
              " 'fun_',\n",
              " 'completely_',\n",
              " 'st',\n",
              " 'seeing_',\n",
              " 'us',\n",
              " 'te',\n",
              " 'special_',\n",
              " 'ation_',\n",
              " 'as',\n",
              " 'ive_',\n",
              " 'ful_',\n",
              " 'read_',\n",
              " 'reason_',\n",
              " 'co',\n",
              " 'need_',\n",
              " 'sa',\n",
              " 'true_',\n",
              " 'ted_',\n",
              " 'like',\n",
              " 'ck',\n",
              " 'place_',\n",
              " 'they',\n",
              " '10_',\n",
              " 'However',\n",
              " 'until_',\n",
              " 'rest_',\n",
              " 'sense_',\n",
              " 'ity_',\n",
              " 'everything_',\n",
              " 'people',\n",
              " 'nt',\n",
              " 'ending_',\n",
              " 'again_',\n",
              " 'ers_',\n",
              " 'given_',\n",
              " 'idea_',\n",
              " 'let_',\n",
              " 'nice_',\n",
              " 'help_',\n",
              " 'no',\n",
              " 'truly_',\n",
              " 'beautiful_',\n",
              " 'ter',\n",
              " 'ck_',\n",
              " 'version_',\n",
              " 'try_',\n",
              " 'came_',\n",
              " 'Even_',\n",
              " 'DVD_',\n",
              " 'se',\n",
              " 'mis',\n",
              " 'scene',\n",
              " 'job_',\n",
              " 'ting_',\n",
              " 'Me',\n",
              " 'At_',\n",
              " 'who',\n",
              " 'money_',\n",
              " 'ment',\n",
              " 'ch',\n",
              " 'recommend_',\n",
              " 'was',\n",
              " 'once_',\n",
              " 'getting_',\n",
              " 'tell_',\n",
              " 'de_',\n",
              " 'gives_',\n",
              " 'not',\n",
              " 'Lo',\n",
              " 'we',\n",
              " 'son',\n",
              " 'shot_',\n",
              " 'second_',\n",
              " 'After_',\n",
              " 'To_',\n",
              " 'high_',\n",
              " 'screen_',\n",
              " ' -- ',\n",
              " 'keep_',\n",
              " 'felt_',\n",
              " 'with',\n",
              " 'great',\n",
              " 'everyone_',\n",
              " 'although_',\n",
              " 'poor_',\n",
              " 'el',\n",
              " 'half_',\n",
              " 'playing_',\n",
              " 'couple_',\n",
              " 'now',\n",
              " 'ble',\n",
              " 'excellent_',\n",
              " 'enjoy_',\n",
              " 'couldn',\n",
              " 'x_',\n",
              " 'ne_',\n",
              " ',\" ',\n",
              " 'ie_',\n",
              " 'go',\n",
              " 'become_',\n",
              " 'less',\n",
              " 'himself_',\n",
              " 'supposed_',\n",
              " 'won',\n",
              " 'understand_',\n",
              " 'seen',\n",
              " 'ally_',\n",
              " 'THE_',\n",
              " 'se_',\n",
              " 'actor_',\n",
              " 'ts_',\n",
              " 'small_',\n",
              " 'line_',\n",
              " 'na',\n",
              " 'audience_',\n",
              " 'fan_',\n",
              " 'et',\n",
              " 'world',\n",
              " 'entire_',\n",
              " 'said_',\n",
              " 'at',\n",
              " '3_',\n",
              " 'scenes',\n",
              " 'rs_',\n",
              " 'full_',\n",
              " 'year_',\n",
              " 'men_',\n",
              " 'ke',\n",
              " 'doing_',\n",
              " 'went_',\n",
              " 'director',\n",
              " 'back',\n",
              " 'early_',\n",
              " 'Hollywood_',\n",
              " 'start_',\n",
              " 'liked_',\n",
              " 'against_',\n",
              " 'remember_',\n",
              " 'love',\n",
              " 'He',\n",
              " 'along_',\n",
              " 'ic',\n",
              " 'His_',\n",
              " 'wife_',\n",
              " 'effects_',\n",
              " 'together_',\n",
              " 'ch_',\n",
              " 'Ra',\n",
              " 'ty',\n",
              " 'maybe_',\n",
              " 'age',\n",
              " 'S_',\n",
              " 'While_',\n",
              " 'often_',\n",
              " 'sort_',\n",
              " 'definitely_',\n",
              " 'No',\n",
              " 'script',\n",
              " 'times',\n",
              " 'absolutely_',\n",
              " 'book_',\n",
              " 'day',\n",
              " 'human_',\n",
              " 'There',\n",
              " 'top_',\n",
              " 'ta',\n",
              " 'becomes_',\n",
              " 'piece_',\n",
              " 'waste_',\n",
              " 'seemed_',\n",
              " 'down',\n",
              " '5_',\n",
              " 'later_',\n",
              " 'rs',\n",
              " 'ja',\n",
              " 'certainly_',\n",
              " 'budget_',\n",
              " 'th',\n",
              " 'nce_',\n",
              " '200',\n",
              " '. (',\n",
              " 'age_',\n",
              " 'next_',\n",
              " 'ar',\n",
              " 'several_',\n",
              " 'ling_',\n",
              " 'short_',\n",
              " 'sh',\n",
              " 'fe',\n",
              " 'Of_',\n",
              " 'instead_',\n",
              " 'Man',\n",
              " 'T_',\n",
              " 'right',\n",
              " 'father_',\n",
              " 'actors',\n",
              " 'wanted_',\n",
              " 'cast',\n",
              " 'black_',\n",
              " 'Don',\n",
              " 'more',\n",
              " '1_',\n",
              " 'comedy',\n",
              " 'better',\n",
              " 'camera_',\n",
              " 'wonderful_',\n",
              " 'production_',\n",
              " 'inter',\n",
              " 'course',\n",
              " 'low_',\n",
              " 'else_',\n",
              " 'w_',\n",
              " 'ness',\n",
              " 'course_',\n",
              " 'based_',\n",
              " 'ti',\n",
              " 'Some_',\n",
              " 'know',\n",
              " 'house_',\n",
              " 'say',\n",
              " 'de',\n",
              " 'watch',\n",
              " 'ous',\n",
              " 'pro',\n",
              " 'tries_',\n",
              " 'ra',\n",
              " 'kids_',\n",
              " 'etc',\n",
              " ' \\x96 ',\n",
              " 'loved_',\n",
              " 'est_',\n",
              " 'fun',\n",
              " 'made',\n",
              " 'video_',\n",
              " 'un',\n",
              " 'totally_',\n",
              " 'Michael_',\n",
              " 'ho',\n",
              " 'mind_',\n",
              " 'No_',\n",
              " 'Be',\n",
              " 'ive',\n",
              " 'La',\n",
              " 'Fi',\n",
              " 'du',\n",
              " 'ers',\n",
              " 'Well',\n",
              " 'wants_',\n",
              " 'How_',\n",
              " 'series',\n",
              " 'performances_',\n",
              " 'written_',\n",
              " 'live_',\n",
              " 'New_',\n",
              " 'So',\n",
              " 'Ne',\n",
              " 'Na',\n",
              " 'night_',\n",
              " 'ge',\n",
              " 'gave_',\n",
              " 'home_',\n",
              " 'heart',\n",
              " 'women_',\n",
              " 'nu',\n",
              " 'ss_',\n",
              " 'hope_',\n",
              " 'ci',\n",
              " 'friends_',\n",
              " 'Se',\n",
              " 'years',\n",
              " 'sub',\n",
              " 'head_',\n",
              " 'Y_',\n",
              " 'Du',\n",
              " '. \"',\n",
              " 'turn_',\n",
              " 'red_',\n",
              " 'perfect_',\n",
              " 'already_',\n",
              " 'classic_',\n",
              " 'tri',\n",
              " 'ss',\n",
              " 'person_',\n",
              " 'star_',\n",
              " 'screen',\n",
              " 'style_',\n",
              " 'ur',\n",
              " 'starts_',\n",
              " 'under_',\n",
              " 'Then_',\n",
              " 'ke_',\n",
              " 'ine',\n",
              " 'ies',\n",
              " 'um',\n",
              " 'ie',\n",
              " 'face_',\n",
              " 'ir',\n",
              " 'enjoyed_',\n",
              " 'point',\n",
              " 'lines_',\n",
              " 'Mr',\n",
              " 'turns_',\n",
              " 'what',\n",
              " 'side_',\n",
              " 'sex_',\n",
              " 'Ha',\n",
              " 'final_',\n",
              " ').<',\n",
              " 'With_',\n",
              " 'care_',\n",
              " 'tion_',\n",
              " 'She',\n",
              " 'ation',\n",
              " 'Ar',\n",
              " 'ma',\n",
              " 'problem_',\n",
              " 'lost_',\n",
              " 'are',\n",
              " 'li',\n",
              " '4_',\n",
              " 'fully_',\n",
              " 'oo',\n",
              " 'sha',\n",
              " 'Just_',\n",
              " 'name_',\n",
              " 'ina',\n",
              " 'boy_',\n",
              " 'finally_',\n",
              " 'ol',\n",
              " '!<',\n",
              " 'Bo',\n",
              " 'about',\n",
              " 'though',\n",
              " 'hand',\n",
              " 'ton',\n",
              " 'lead_',\n",
              " 'school_',\n",
              " 'ns',\n",
              " 'ha',\n",
              " 'favorite_',\n",
              " 'stupid_',\n",
              " 'gi',\n",
              " 'original',\n",
              " 'mean_',\n",
              " 'To',\n",
              " 'took_',\n",
              " 'either_',\n",
              " 'ni',\n",
              " 'book',\n",
              " 'episode_',\n",
              " 'om',\n",
              " 'Su',\n",
              " 'D_',\n",
              " 'Mc',\n",
              " 'house',\n",
              " 'cannot_',\n",
              " 'stars_',\n",
              " 'behind_',\n",
              " 'see',\n",
              " 'other',\n",
              " 'Che',\n",
              " 'role',\n",
              " 'art',\n",
              " 'ever',\n",
              " 'Why_',\n",
              " 'father',\n",
              " 'case_',\n",
              " 'tic_',\n",
              " 'moments_',\n",
              " 'Co',\n",
              " 'works_',\n",
              " 'sound_',\n",
              " 'Ta',\n",
              " 'guess_',\n",
              " 'perhaps_',\n",
              " 'Vi',\n",
              " 'thing',\n",
              " 'fine_',\n",
              " 'fact',\n",
              " 'music',\n",
              " 'non',\n",
              " 'ful',\n",
              " 'action',\n",
              " 'ity',\n",
              " 'ct',\n",
              " 'ate_',\n",
              " 'type_',\n",
              " 'lack_',\n",
              " 'death_',\n",
              " 'art_',\n",
              " 'able',\n",
              " 'Ja',\n",
              " 'ge_',\n",
              " 'wouldn',\n",
              " 'am',\n",
              " 'tor',\n",
              " 'extremely_',\n",
              " 'pre',\n",
              " 'self',\n",
              " 'Mor',\n",
              " 'particularly_',\n",
              " 'bo',\n",
              " 'est',\n",
              " 'Ba',\n",
              " 'ya',\n",
              " 'play',\n",
              " 'Pa',\n",
              " 'ther',\n",
              " 'heard_',\n",
              " 'however',\n",
              " 'ver',\n",
              " 'dy_',\n",
              " 'Sa',\n",
              " 'ding_',\n",
              " 'led_',\n",
              " 'late_',\n",
              " 'feeling_',\n",
              " 'per',\n",
              " 'low',\n",
              " 'ably_',\n",
              " 'Un',\n",
              " 'On_',\n",
              " 'known_',\n",
              " 'kill_',\n",
              " 'fight_',\n",
              " 'beginning_',\n",
              " 'cat',\n",
              " 'bit',\n",
              " 'title_',\n",
              " 'vo',\n",
              " 'short',\n",
              " 'old',\n",
              " 'including_',\n",
              " 'Da',\n",
              " 'coming_',\n",
              " 'That',\n",
              " 'place',\n",
              " 'looked_',\n",
              " 'best',\n",
              " 'Lu',\n",
              " 'ent_',\n",
              " 'bla',\n",
              " 'quality_',\n",
              " 'except_',\n",
              " '...<',\n",
              " 'ff',\n",
              " 'decent_',\n",
              " 'much',\n",
              " 'De',\n",
              " 'Bu',\n",
              " 'ter_',\n",
              " 'attempt_',\n",
              " 'Bi',\n",
              " 'taking_',\n",
              " 'ig',\n",
              " 'Ti',\n",
              " 'whose_',\n",
              " 'dialogue_',\n",
              " 'zz',\n",
              " 'war_',\n",
              " 'ill',\n",
              " 'Te',\n",
              " 'war',\n",
              " 'Hu',\n",
              " 'James_',\n",
              " '..',\n",
              " 'under',\n",
              " 'ring_',\n",
              " 'pa',\n",
              " 'ot',\n",
              " 'expect_',\n",
              " 'Ga',\n",
              " 'itself_',\n",
              " 'line',\n",
              " 'lives_',\n",
              " 'let',\n",
              " 'Dr',\n",
              " 'mp',\n",
              " 'che',\n",
              " 'mean',\n",
              " 'called_',\n",
              " 'complete_',\n",
              " 'terrible_',\n",
              " 'boring_',\n",
              " 'others_',\n",
              " '\" (',\n",
              " 'aren',\n",
              " 'star',\n",
              " 'long',\n",
              " 'Li',\n",
              " 'mother_',\n",
              " 'si',\n",
              " 'highly_',\n",
              " 'ab',\n",
              " 'ex',\n",
              " 'os',\n",
              " 'nd',\n",
              " 'ten_',\n",
              " 'ten',\n",
              " 'run_',\n",
              " 'directed_',\n",
              " 'town_',\n",
              " 'friend_',\n",
              " 'David_',\n",
              " 'taken_',\n",
              " 'finds_',\n",
              " 'fans_',\n",
              " 'Mar',\n",
              " 'writing_',\n",
              " 'white_',\n",
              " 'u_',\n",
              " 'obviously_',\n",
              " 'mar',\n",
              " 'Ho',\n",
              " 'year',\n",
              " 'stop_',\n",
              " 'f_',\n",
              " 'leave_',\n",
              " 'king_',\n",
              " 'act_',\n",
              " 'mind',\n",
              " 'entertaining_',\n",
              " 'ish_',\n",
              " 'Ka',\n",
              " 'throughout_',\n",
              " 'viewer_',\n",
              " 'despite_',\n",
              " 'Robert_',\n",
              " 'somewhat_',\n",
              " 'hour_',\n",
              " 'car_',\n",
              " 'evil_',\n",
              " 'Although_',\n",
              " 'wrong_',\n",
              " 'Ro',\n",
              " 'dead_',\n",
              " 'body_',\n",
              " 'awful_',\n",
              " 'home',\n",
              " 'exactly_',\n",
              " 'bi',\n",
              " 'family',\n",
              " 'ts',\n",
              " 'usually_',\n",
              " 'told_',\n",
              " 'z_',\n",
              " 'oc',\n",
              " 'minutes',\n",
              " 'tra',\n",
              " 'some',\n",
              " 'actor',\n",
              " 'den',\n",
              " 'but',\n",
              " 'Sha',\n",
              " 'tu',\n",
              " 'strong_',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6Zv42opKuRH",
        "outputId": "848b6f30-25e9-4838-e24d-afdf60714400",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "model = Seqeuntial([\n",
        "    Embedding(input_dim=vocab_size, output_dim=output_dim, input_dim=input_length) ,\n",
        "    GlobalAveragePooling1D(),\n",
        "    Dense(),\n",
        "    Dense()\n",
        "], name='IMDB CLassifier')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-7bbd1915ae2e>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    Embedding(input_dim=vocab_size, output_dim=output_dim, input_dim=input_length) ,\u001b[0m\n\u001b[0m                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m keyword argument repeated\n"
          ]
        }
      ]
    }
  ]
}